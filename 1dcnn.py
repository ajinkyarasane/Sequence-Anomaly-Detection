# -*- coding: utf-8 -*-
"""Train-1dCNNipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10T-hYZpCXwL2DR9AAQJiQXmOpslN9a2y
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.cuda.amp as amp
from sklearn.metrics import accuracy_score
import os

from google.colab import drive
drive.mount("/content/gdrive", force_remount=True)

#below commented code is to bring data to your disk
#you can change paths according to your wish however they must be consistent through out the code
# ! cp /content/gdrive/MyDrive/Train_data.csv .
# ! cp /content/gdrive/MyDrive/vocabulary.csv .
# ! cp /content/gdrive/MyDrive/New-FSD50K-Dataset/Ambi_pann.zip .
# ! cp /content/gdrive/MyDrive/New-FSD50K-Dataset/Human_pann.zip .
# ! cp /content/gdrive/MyDrive/New-FSD50K-Dataset/Music_pann.zip .
# ! cp /content/gdrive/MyDrive/New-FSD50K-Dataset/Nature_pann.zip .
# ! cp /content/gdrive/MyDrive/New-FSD50K-Dataset/SoT_pann-001.zip .
# ! cp /content/gdrive/MyDrive/New-FSD50K-Dataset/SoT_pann-002.zip .
# ! cp /content/gdrive/MyDrive/New-FSD50K-Dataset/SoT_pann-003.zip .
# ! cp /content/gdrive/MyDrive/New-FSD50K-Dataset/Animal_pann.zip .

# ! unzip Ambi_pann.zip
# ! unzip Animal_pann.zip
# ! unzip Human_pann.zip
# ! unzip Music_pann.zip
# ! unzip Nature_pann.zip
# ! unzip SoT_pann-001.zip -d /content/content/gdrive/MyDrive/New-FSD50K-Dataset
# ! unzip SoT_pann-002.zip -d /content/content/gdrive/MyDrive/New-FSD50K-Dataset
# ! unzip SoT_pann-003.zip -d /content/content/gdrive/MyDrive/New-FSD50K-Dataset

df=pd.read_csv("/content/Final-Original-Sounds.csv")
#

df=df.iloc[:,1:]

import os
Ambi_Npy=sorted(os.listdir("/content/content/gdrive/MyDrive/New-FSD50K-Dataset/Ambi_pann"))
Animal_Npy=sorted(os.listdir("/content/content/gdrive/MyDrive/New-FSD50K-Dataset/Animal_pann"))
Human_Npy=sorted(os.listdir("/content/content/gdrive/MyDrive/New-FSD50K-Dataset/Human_pann"))
Music_Npy=sorted(os.listdir("/content/content/gdrive/MyDrive/New-FSD50K-Dataset/Music_pann"))
Nature_Npy=sorted(os.listdir("/content/content/gdrive/MyDrive/New-FSD50K-Dataset/Nature_pann"))
SoT_Npy=sorted(os.listdir("/content/content/gdrive/MyDrive/New-FSD50K-Dataset/SoT_pann"))
normal = sorted(os.listdir("/content/gdrive/MyDrive/New-FSD50K-Dataset/Original_pann"))
x_vec=[]
X_path="/content/content/gdrive/MyDrive/New-FSD50K-Dataset/"
for j,i in df.Fname.iteritems():
  k=i+".npy"
  if k in Ambi_Npy:
    x_vec.append(i)
  if k in Animal_Npy:
    x_vec.append(i)
  if k in Human_Npy:
    x_vec.append(i)
  if k in Music_Npy:
    x_vec.append(i)
  if k in Nature_Npy:
    x_vec.append(i)
  if k in SoT_Npy:
    x_vec.append(i)
  if k in normal:
    x_vec.append(i)

vocab=pd.read_csv("/content/vocabulary.csv",header=None)

vocab.columns=["ind","tags","ids"]

vocab["tags"]=vocab["tags"].str.lower()

hot=[]
for i,k in df.Anomaly_Sound_Audioset_Tags.iteritems():
  ones=[0 for z in range(201)]
  # print(k)
  if k=="None":
    ones[200]=1
    hot.append(ones)
    continue
  for j in k.lower().split(","):
    row_nos=vocab["tags"][vocab["tags"]==j].index
    for t in row_nos:
      ones[t]=1  
  hot.append(ones)

True_Natural = ['Wind','Thunderstorm','Thunder','Water','Rain','Raindrop','Stream','Ocean','Waves_and_surf','Gurgling','Fire']

True_Ambigous = ['Hiss','Chirp_and_tweet','Buzz','Rattle','Crackle','Knock','Tap','Squeak','Tick','Crack','Whoosh_and_swoosh_and_swish','Thump_and_thud','Crushing','Crumpling_and_crinkling','Tearing',
'Screech']

True_Hs  = ['Human_voice','Speech','Male_speech_and_man_speaking','Female_speech_and_woman_speaking','Child_speech_and_kid_speaking','Conversation','Speech_synthesizer', \
    'Shout','Yell','Screaming','Whispering','Laughter','Giggle','Chuckle_and_chortle','Crying_and_sobbing','Sigh','Singing','Male_singing','Female_singing','Respiratory_sounds', \
    'Breathing','Gasp','Cough','Sneeze','Run','Walk_and_footsteps','Chewing_and_mastication','Burping_and_eructation','Fart','Hands','Finger_snapping','Clapping','Human_group_actions','Cheering', \
    'Applause','Chatter','Crowd']

True_SoT = ['Cowbell','Bell','Church_bell','Bicycle_bell','Chime','Wind_chime','Vehicle','Boat_and_Water_vehicle','Motor_vehicle_(road)','Car','Vehicle_horn_and_car_horn_and_honking', \
    'Car_passing_by','Race_car_and_auto_racing','Truck','Bus','Motorcycle','Traffic_noise_and_roadway_noise','Rail_transport','Train','Subway_and_metro_and_underground', \
    'Aircraft','Fixed-wing_aircraft_and_airplane','Bicycle','Skateboard','Engine','Engine_starting','Idling','Accelerating_and_revving_and_vroom','Domestic_sounds_and_home_sounds', \
    'Door','Doorbell','Sliding_door','Slam','Cupboard_open_or_close','Drawer_open_or_close','Dishes_and_pots_and_pans','Cutlery_and_silverware','Frying_(food)','Microwave_oven', \
    'Water_tap_and_faucet','Sink_(filling_or_washing)','Bathtub_(filling_or_washing)','Toilet_flush','Zipper_(clothing)','Keys_jangling','Coin_(dropping)','Packing_tape_and_duct_tape','Scissors', \
    'Typing','Typewriter','Computer_keyboard','Writing','Alarm','Telephone','Ringtone','Siren','Mechanisms','Ratchet_and_pawl','Clock','Tick-tock','Mechanical_fan','Printer', \
    'Camera','Tools','Hammer','Sawing','Power_tool','Drill','Explosion','Gunshot_and_gunfire','Fireworks','Boom','Wood','Glass','Chink_and_clink','Shatter','Liquid','Splash_and_splatter', \
    'Drip','Pour','Trickle_and_dribble','Fill_(with_liquid)','Boiling']

True_A = ['Animal','Domestic_animals_and_pets','Dog','Bark','Growling','Cat','Purr','Meow','Livestock_and_farm_animals_and_working_animals','Fowl','Chicken_and_rooster', \
    'Wild_animals','Bird','Bird_vocalization_and_bird_call_and_bird_song','Crow','Gull_and_seagull','Insect','Cricket', 'Frog']

True_M = ['Music','Musical_instrument','Plucked_string_instrument','Guitar','Electric_guitar','Bass_guitar','Acoustic_guitar','Strum','Keyboard_(musical)','Piano', \
'Organ','Percussion','Drum_kit','Drum','Snare_drum','Bass_drum','Tabla','Cymbal','Hi-hat','Crash_cymbal','Tambourine','Rattle_(instrument)','Gong','Mallet_percussion', \
'Marimba_and_xylophone','Glockenspiel','Brass_instrument','Trumpet','Bowed_string_instrument','Wind_instrument_and_woodwind_instrument','Harp','Harmonica','Accordion',
 'Scratching_(performance_technique)']

for i in range(len(True_Natural)):
  True_Natural[i] = True_Natural[i].lower()
for i in range(len(True_Ambigous)):
  True_Ambigous[i] = True_Ambigous[i].lower()
for i in range(len(True_SoT)):
  True_SoT[i] = True_SoT[i].lower()
for i in range(len(True_Hs)):
  True_Hs[i] = True_Hs[i].lower()
for i in range(len(True_A)):
  True_A[i] = True_A[i].lower()
for i in range(len(True_M)):
  True_M[i] = True_M[i].lower()
  #

category=[]
for i,k in df.Anomaly_Sound_Audioset_Tags.iteritems():
  local=[0 for z in range(7)]
  # print(i,k)
  # print(ast.literal_eval(k))
  # print(df["Fname"][i])
  if(k=="None"):
    local[6]=1
    category.append(local)
    continue

  for j in k.lower().split(","):
    if j in True_Natural:
      local[0]=1
    if j in True_Ambigous:
      local[1]=1
    if j in True_Hs:
      local[2]=1
    if j in True_A:
      local[3]=1
    if j in True_SoT:
      local[4]=1
    if j in True_M:
      local[5]=1
  category.append(local)
#

df["y_category"]=category
df["y_hot"]=hot

df=df.drop(df.index[[3069,3070]])
df=df.drop(df.index[df['Fname'] == "110reusschrikt_CrackingKnuckles"])

# df2=df2.reset_index(drop=True)
df2=df[df["Fname"].isin(x_vec)]

#sample data if needed
df2=df2.sample(n=5000)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df2.iloc[:,:-2], df2.iloc[:,-2:], test_size=0.2, random_state=1,shuffle=True)

class dataload(torch.utils.data.Dataset):
  def __init__(self,train_ids,y_labs):
    self.Ambi_Npy=sorted(os.listdir("/content/content/gdrive/MyDrive/New-FSD50K-Dataset/Ambi_pann"))
    self.Animal_Npy=sorted(os.listdir("/content/content/gdrive/MyDrive/New-FSD50K-Dataset/Animal_pann"))
    self.Human_Npy=sorted(os.listdir("/content/content/gdrive/MyDrive/New-FSD50K-Dataset/Human_pann"))
    self.Music_Npy=sorted(os.listdir("/content/content/gdrive/MyDrive/New-FSD50K-Dataset/Music_pann"))
    self.Nature_Npy=sorted(os.listdir("/content/content/gdrive/MyDrive/New-FSD50K-Dataset/Nature_pann"))
    self.SoT_Npy=sorted(os.listdir("/content/content/gdrive/MyDrive/New-FSD50K-Dataset/SoT_pann"))
    self.norm = sorted(os.listdir("/content/gdrive/MyDrive/New-FSD50K-Dataset/Original_pann"))
    self.x_vec=train_ids
    self.length=len(self.x_vec)
    self.y_vec=y_labs
    self.x_dat=[]

    
  def __len__(self):
    return self.length

  def __getitem__(self,i):
    X_path="/content/content/gdrive/MyDrive/New-FSD50K-Dataset/"
    fname=self.x_vec[i]
    k=fname+".npy"
    # print(i)
    x=None
    if k in self.Ambi_Npy:
      x=np.load(X_path+"Ambi_pann/"+k)
      # return x,self.y_vec[i]
      # self.x_dat.append(x)
    if k in self.Animal_Npy:
      x=np.load(X_path+"Animal_pann/"+k)
      # return x,self.y_vec[i]
      # self.x_dat.append(x)
    if k in self.Human_Npy:
      x=np.load(X_path+"Human_pann/"+k)
      # return x,self.y_vec[i]
      # self.x_dat.append(x)
    if k in self.Music_Npy:
      x=np.load(X_path+"Music_pann/"+k)
      # return x,self.y_vec[i]
      # self.x_dat.append(x)
    if k in self.Nature_Npy:
      x=np.load(X_path+"Nature_pann/"+k)
      # return x,self.y_vec[i]
      # self.x_dat.append(x)
    if k in self.SoT_Npy:
      x=np.load(X_path+"/SoT_pann/"+k)
      # return x,self.y_vec[i]
      # self.x_dat.append(x)
    if k in self.norm:
      x=np.load("/content/gdrive/MyDrive/New-FSD50K-Dataset/Original_pann/"+k)
    pd=6955-x.shape[0]
    t=np.zeros((pd,527))
    x=np.concatenate((x,t),axis=0)
    return x,self.y_vec[i]
    #

import torch.nn.functional as Fx
import torch.nn.init as init
class NET1D(nn.Module):

    def __init__(self,nclass):
        super(NET1D,self).__init__()
        self.globalpool = Fx.avg_pool1d

        self.layer1 = nn.Sequential(nn.Conv1d(527,64,kernel_size=5,padding=1, stride=2),nn.BatchNorm1d(64),nn.ReLU())
        self.layer2 = nn.Sequential(nn.Conv1d(64,64,kernel_size=5,padding=1, stride=2),nn.BatchNorm1d(64),nn.ReLU())
        self.layer3 = nn.MaxPool1d(2) 

        self.layer4 = nn.Sequential(nn.Conv1d(64,128,kernel_size=5,padding=1, stride=2),nn.BatchNorm1d(128),nn.ReLU())
        self.layer5 = nn.Sequential(nn.Conv1d(128,128,kernel_size=5,padding=1, stride=2),nn.BatchNorm1d(128),nn.ReLU())
        self.layer6 = nn.MaxPool1d(2) 

        self.layer7 = nn.Sequential(nn.Conv1d(128,64,kernel_size=5,padding=1, stride=2),nn.BatchNorm1d(64),nn.ReLU())
        self.layer8 = nn.Sequential(nn.Conv1d(64,64,kernel_size=5,padding=1, stride=2),nn.BatchNorm1d(64),nn.ReLU())
        # self.layer9 = nn.MaxPool2d((1,2))
        self.layer15 = nn.Sequential(nn.Conv1d(64,nclass,kernel_size=1))

        # self.layer10 = nn.Sequential(nn.Conv2d(256,512,kernel_size=3,padding=1),nn.BatchNorm2d(512),nn.ReLU())
        # self.layer11 = nn.Sequential(nn.Conv2d(512,512,kernel_size=3,padding=1),nn.BatchNorm2d(512),nn.ReLU())
        # self.layer12 = nn.MaxPool2d((1,2)) 
        

        # self.layer13 = nn.Sequential(nn.Conv2d(512,1024,kernel_size=(1,8)),nn.BatchNorm2d(1024),nn.ReLU())
        # self.layer14 = nn.Sequential(nn.Conv2d(1024,1024,kernel_size=1),nn.BatchNorm2d(1024),nn.ReLU())
        # self.layer15 = nn.Sequential(nn.Conv2d(1024,nclass,kernel_size=1))
        
        # self.lin=nn.Sequential(nn.Linear(200,100),nn.ReLU(),nn.Linear(100,20),nn.ReLU(),nn.Linear(20,6))
    def forward(self,x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = self.layer5(out)
        out = self.layer6(out)
        out = self.layer7(out)
        out = self.layer8(out)
        # out = self.layer9(out)
        # out = self.layer10(out)
        # out = self.layer11(out)
        # out = self.layer12(out)
        # out = self.layer13(out)
        # out = self.layer14(out)
        out1 = self.layer15(out)
        
        out = self.globalpool(out1,kernel_size=out1.size()[2:])
        out = out.view(out.size(0),-1)
        # out=self.lin(out)
        return out #,out1

    # def xavier_init(self):
    #     for m in self.modules():
    #         if isinstance(m, nn.Conv2d):
    #             init.xavier_uniform(m.weight, gain=nn.init.calculate_gain('relu'))
    #             m.bias.data.zero_()
    #         elif isinstance(m, nn.BatchNorm2d):
    #             m.weight.data.fill_(1)
    #             m.bias.data.zero_()

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
def calculate_metrics(pred, target, threshold=0.5):
  pred = np.array(pred > threshold, dtype=int)
  # print(pred,target)
  print('Accuracy : ', accuracy_score(y_true=target, y_pred=pred))
  print('micro/precision', precision_score(y_true=target, y_pred=pred, average='micro'))
  print('micro/recall', recall_score(y_true=target, y_pred=pred, average='micro'))
  print('micro/f1', f1_score(y_true=target, y_pred=pred, average='micro'))
  print('macro/precision', precision_score(y_true=target, y_pred=pred, average='macro'))
  print('macro/recall', recall_score(y_true=target, y_pred=pred, average='macro'))
  print('macro/f1', f1_score(y_true=target, y_pred=pred, average='macro'))
  a = accuracy_score(y_true=target, y_pred=pred)
  b = precision_score(y_true=target, y_pred=pred, average='micro')
  c = recall_score(y_true=target, y_pred=pred, average='micro')
  d = f1_score(y_true=target, y_pred=pred, average='micro')
  e = precision_score(y_true=target, y_pred=pred, average='macro')
  f = recall_score(y_true=target, y_pred=pred, average='macro')
  g = f1_score(y_true=target, y_pred=pred, average='macro')
  return a,b,c,d,e,f,g
#

import os
train_dataset=dataload(X_train["Fname"].tolist(),np.array(y_train["y_hot"].to_list()))
# print(len(X_train["Fname"].tolist()))
# train_loader = torch.utils.data.DataLoader(train_data, batch_size=16,shuffle=True)
val_dataset=dataload(X_test["Fname"].tolist(),np.array(y_test["y_hot"].to_list()))
# val_loader = torch.utils.data.DataLoader(val_data, batch_size=16,shuffle=False)
# train_dataset=dataload(train_data.x_dat,np.array(y_train.to_list()))
# val_dataset=dataload(test_data.x_dat,np.array(y_test.to_list()))

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128,shuffle=True)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1,shuffle=False)

torch.cuda.empty_cache()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model = Network().to(device)
# model = NET(6).to(device)
model = NET1D(201).to(device)
final_accuracy_score = []
micro_precision_score = []
micro_recall_score = []
micro_f1_score = []
macro_precision_score = []
macro_recall_score = []
macro_f1_score = []
average_loss = []
# model.xavier_init()
epochs=30
# model.load_state_dict(dict_param['model_state_dict'])
# optimizer = optim.Adam(model.parameters(), lr=0.001)
# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2,verbose=True)
# scaler=amp.GradScaler()
# optimizer.load_state_dict(dict_param['potimizer_state_dict'])
# for g in optimizer.param_groups:
#   g['lr'] = 0.003
criterion = torch.nn.BCEWithLogitsLoss()
sig=nn.Sigmoid()

def val_score():
  model.eval()
  with torch.no_grad():
    model_result = []
    targets=[]
    for i, (data,target) in enumerate(val_loader):
      # data = data.float().reshape(data.shape[0],1,data.shape[1],data.shape[2]).to(device)
      transposed_data = torch.zeros([data.shape[0], data.shape[2], data.shape[1]], dtype=torch.float32)
      for i in range(data.shape[0]):
      # print(data[i].t().shape)
        transposed_data[i] = data[i].t()
      # print(data.shape)
      transposed_data = transposed_data.float().to(device)
      # data = data.float()
      # for i in range(data.shape[0]):
      #   data[i] = data[i].t()
      # print(data.shape)
      target = target.float().to(device)
      output = model(transposed_data)
      output = sig(output)
      # print(output.shape)
      model_result.extend(output.cpu().numpy())
      targets.extend(target.cpu().numpy())
    a,b,c,d,e,f,g = calculate_metrics(np.array(model_result), np.array(targets))
    final_accuracy_score.append(a)
    micro_precision_score.append(b)
    micro_recall_score.append(c)
    micro_f1_score.append(d)
    macro_precision_score.append(e)
    macro_recall_score.append(f)
    macro_f1_score.append(g)


for ep in range(epochs):
  model.train()
  avg_loss=0
  for i, (data,target) in enumerate(train_loader):
    torch.cuda.empty_cache()
    transposed_data = torch.zeros([data.shape[0], data.shape[2], data.shape[1]], dtype=torch.float32)
    # print(data.shape)
    for i in range(data.shape[0]):
      # print(data[i].t().shape)
      transposed_data[i] = data[i].t()
    # print(data.shape)
    transposed_data = transposed_data.float().to(device)

    optimizer.zero_grad()
    output = model(transposed_data)
    del data
    target = target.float().to(device)
    loss = criterion(output, target)
    del target
    # print(loss)
    # print(loss)
    avg_loss+=loss
    loss.backward()
    optimizer.step()
    torch.cuda.empty_cache()
  print("============EPOCH : ",ep,"=======================")
  print("average loss epoch: ",avg_loss)
  print("average loss epoch: ",avg_loss/len(train_dataset))
  average_loss.append(avg_loss/len(train_loader))
  val_score()
  print("==============================================")
  scheduler.step(avg_loss)

import matplotlib.pyplot as plt
plt.plot(np.arange(epochs),final_accuracy_score)
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Accuracy vs Epochs')
plt.show()
plt.plot(np.arange(epochs),micro_precision_score)
plt.xlabel('Epochs')
plt.ylabel('Micro Precision')
plt.title('Micro Precision vs Epochs')
plt.show()
plt.plot(np.arange(epochs),micro_recall_score)
plt.xlabel('Epochs')
plt.ylabel('Micro Recall')
plt.title('Micro Recall vs Epochs')
plt.show()
plt.plot(np.arange(epochs),micro_f1_score)
plt.xlabel('Epochs')
plt.ylabel('F1')
plt.title('F1 score vs Epochs')
plt.show()
plt.plot(np.arange(epochs),macro_precision_score)
plt.xlabel('Epochs')
plt.ylabel('Macro Precision')
plt.title('Macro Precision vs Epochs')
plt.show()
plt.plot(np.arange(epochs),macro_recall_score)
plt.xlabel('Epochs')
plt.ylabel('Macro Recall')
plt.title('Macro Recall vs Epochs')
plt.show()
plt.plot(np.arange(epochs),macro_f1_score)
plt.xlabel('Epochs')
plt.ylabel('Macro F1')
plt.title('Macro F1 vs Epochs')
plt.show()

model_name='1dcnn-201.pt'
model_save_path=F"/content/gdrive/My Drive/{model_name}"
torch.save({'epoch':ep,
            'model_state_dict':model.state_dict(),
            'potimizer_state_dict':optimizer.state_dict(),
            }, model_save_path)

